{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pathlib import Path\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/zhouan/App/BenchmarkingClassifiers/snakemake/config/classifiers.yml.template\", 'r') as f:\n",
    "    locations = yaml.load(f, Loader=yaml.FullLoader)\n",
    "type(locations)\n",
    "locations[\"sylph\"][\"db_path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_fungus_or_nah(taxids):\n",
    "    base_url = f'https://api.ncbi.nlm.nih.gov/datasets/v2alpha/taxonomy/taxon/'\n",
    "    taxids_str = ','.join(map(str, taxids))\n",
    "    url = f'{base_url}{taxids_str}'\n",
    "    params = {}\n",
    "    try:\n",
    "        response = s.get(url, params=params)\n",
    "        response.raise_for_status() # \n",
    "    except Exception as err:\n",
    "        print(f'Other error occurred: {err}')\n",
    "    else:\n",
    "        r = response.json()\n",
    "    return r\n",
    "fetch_fungus_or_nah(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_reference_dataset_report(taxon, reference_only='true'):\n",
    "    base_url = f'https://api.ncbi.nlm.nih.gov/datasets/v2alpha/genome/taxon/{taxon}/dataset_report?'\n",
    "    params = {f'filters.reference_only': {reference_only},\n",
    "              'filters.exclude_paired_reports': 'false',\n",
    "              'filters.exclude_atypical': 'true'}\n",
    "    try:\n",
    "        response = s.get(base_url, params=params)\n",
    "        # print(response.url)\n",
    "        response.raise_for_status()\n",
    "    except Exception as err:\n",
    "        print(f'Other error occurred: {err}')\n",
    "    else:\n",
    "        return response.json()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for taxid, seq_abund in theoretical_abundance['species'].items():\n",
    "    r = fetch_reference_dataset_report(taxid)\n",
    "\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(r['reports'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_reference_sequence_report(accession):\n",
    "    base_url = f'https://api.ncbi.nlm.nih.gov/datasets/v2alpha/genome/accession/{accession}/sequence_reports?'\n",
    "    params = {}\n",
    "    try:\n",
    "        response = s.get(base_url, params=params)\n",
    "        response.raise_for_status()\n",
    "    except Exception as err:\n",
    "        print(f'Other error occurred: {err}')\n",
    "    else:\n",
    "        return response.json()\n",
    "r = fetch_reference_sequence_report('GCF_000091045.1')\n",
    "r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = {}\n",
    "lengths = 0\n",
    "for sequence in r['reports']:\n",
    "    if sequence['assigned_molecule_location_type'] == 'Plasmid':\n",
    "        pass\n",
    "    elif sequence['assigned_molecule_location_type'] == 'Chromosome':\n",
    "        sequences[sequence['refseq_accession']] = sequence['length']\n",
    "        lengths += sequence['length']\n",
    "sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import math\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pandas as pd\n",
    "from rich.status import Status\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "theoretical_abundances = {}\n",
    "data=\"/home/zhouan/Docker_dir/Classification/00.extract_method/Vazyme_D6300/BenchmarkingClassifiers\"\n",
    "for ground_truth_file in [\"ground_truth_tax_updated.yml\" ,\"ground_truth_updated.yml\"]:\n",
    "    ground_truth_file=Path(f\"{data}/{ground_truth_file}\")\n",
    "    with open(ground_truth_file, 'r') as f:\n",
    "        theoretical_abundance = {'species': yaml.load(f, Loader=yaml.FullLoader)}\n",
    "    theoretical_abundance['genus'] = {}\n",
    "\n",
    "    for k, v in theoretical_abundance['species'].items():\n",
    "        genus_name = k.split()[0]\n",
    "        if genus_name in theoretical_abundance['genus']:\n",
    "            theoretical_abundance['genus'][genus_name] += v\n",
    "        else:\n",
    "            theoretical_abundance['genus'][genus_name] = v\n",
    "\n",
    "    if ground_truth_file.stem.split('_')[-2] == 'tax':\n",
    "        theoretical_abundances['tax'] = theoretical_abundance\n",
    "    else:\n",
    "        theoretical_abundances['sequence'] = theoretical_abundance\n",
    "theoretical_abundances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_metrics = {\n",
    "    'Kraken2': {\n",
    "        'TP': 10,\n",
    "        'FP': 2,\n",
    "        'FN': 3,\n",
    "        'Precision': 0.833,\n",
    "        'Recall': 0.769,\n",
    "        'F1': 0.800\n",
    "    },\n",
    "    'MetaPhlAn': {\n",
    "        'TP': 8,\n",
    "        'FP': 1,\n",
    "        'FN': 4,\n",
    "        'Precision': 0.889,\n",
    "        'Recall': 0.667,\n",
    "        'F1': 0.762\n",
    "    }\n",
    "}\n",
    "import pandas as pd\n",
    "\n",
    "top_table = pd.DataFrame.from_dict(collected_metrics, orient='index').T\n",
    "print(top_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(collected_metrics, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = ['Classified', 'Classified', 'Unclassified', 'Classified']\n",
    "list_uc = ['no_hit',\n",
    "           'ambigious',\n",
    "           'unclassified',\n",
    "           'missing_genus',\n",
    "           'not_distributed',\n",
    "           'rounding_error'\n",
    "           ]\n",
    "import pandas as pd\n",
    "\n",
    "groups_cat = pd.Categorical(groups, categories=['Classified'] + list_uc)\n",
    "print(groups_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups_cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "data=\"/home/zhouan/Docker_dir/Classification/00.extract_method/Vazyme_D6300/BenchmarkingClassifiers/output\"\n",
    "data=Path(data)\n",
    "data.glob(\"/home/zhouan/Docker_dir/Classification/00.extract_method/Vazyme_D6300/BenchmarkingClassifiers/output/species\" + '/' + '*.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import math\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from dataclasses import dataclass\n",
    "from sklearn import metrics\n",
    "from typing import List\n",
    "@dataclass # 通过 @dataclass，可以直接在类中定义属性，而不需要显式编写 __init__ 方法\n",
    "class ClassificationResults:\n",
    "    classifier: str\n",
    "    results: pd.Series\n",
    "    names_unclassified: List[str]\n",
    "\n",
    "    def bundled_classified(self, normalized=False): #  将结果分为两类：已分类（Classified）和未分类（list_uc 中的单元）\n",
    "        \"\"\"\n",
    "        'Classified' --> ground truth + classified but not in ground truth\n",
    "        The rest are in list_uc and are as-is\n",
    "        \"\"\"\n",
    "        groups = np.where(np.isin(self.results.index, list_uc), self.results.index, 'Classified') # 根据 self.results.index 中的分类单元是否在 list_uc 中，将分类单元分为两类：1. 在 list_uc 中的分类单元，保持原样；2. 不在 list_uc 中的分类单元，标记为 'Classified'；返回NumPy 数组\n",
    "        groups_cat = pd.Categorical(groups,\n",
    "                                    categories=['Classified'] + list_uc) # 将分类单元分为 'Classified'和 list_uc 中的类\n",
    "        bundled_classified = self.results.groupby(groups_cat, observed=False).aggregate('sum') # 对'Classified'和 list_uc 的reads数量进行求和\n",
    "        if normalized:\n",
    "            return self._normalize_series(bundled_classified)\n",
    "        return bundled_classified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_uc = ['no_hit', 'ambigious', 'unclassified', 'missing_genus']\n",
    "classifiers_input = [\n",
    "    ClassificationResults(\n",
    "        classifier='kraken2',\n",
    "        results=pd.Series({\n",
    "            'E_coli': 100,\n",
    "            'S_aureus': 200,\n",
    "            'B_subtilis': 50,\n",
    "            'Unclassified': 30,\n",
    "            'no_hit': 10,\n",
    "            'ambigious': 5,\n",
    "            'unclassified': 2,\n",
    "            'missing_genus': 1\n",
    "        }),\n",
    "        names_unclassified=['no_hit', 'ambigious', 'unclassified', 'missing_genus']\n",
    "    ),\n",
    "    ClassificationResults(\n",
    "        classifier='bracken',\n",
    "        results=pd.Series({\n",
    "            'E_coli': 150,\n",
    "            'S_aureus': 180,\n",
    "            'B_subtilis': 70,\n",
    "            'Unclassified': 20,\n",
    "            'no_hit': 10,\n",
    "            'ambigious': 5,\n",
    "            'unclassified': 2,\n",
    "            'missing_genus': 1\n",
    "        }),\n",
    "        names_unclassified=['no_hit', 'ambigious', 'unclassified', 'missing_genus']\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "un_classified = {}\n",
    "list_abundance_classifiers = ['MetaPhlAn3', 'mOTUs2']\n",
    "for classifier_result in classifiers_input:\n",
    "    if classifier_result.classifier not in list_abundance_classifiers: # 如果分类器不直接输出丰度结果\n",
    "        un_classified[classifier_result.classifier] = classifier_result.bundled_classified(False) # 返回一个字典，包含Classified'和 list_uc 分类的reads数量之和\n",
    "un_classified_values = pd.DataFrame.from_dict(un_classified)\n",
    "output_plots_organism=\"/home/zhouan/Docker_dir/Classification/00.extract_method/Vazyme_D6300/BenchmarkingClassifiers/output/species\"\n",
    "output_plots_organism = Path(output_plots_organism)\n",
    "un_classified_values.to_csv(output_plots_organism / 'number_table.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enumerate(classifiers_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MetaPhlAn_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "classification_tax = \"/home/zhouan/Docker_dir/Classification/MetaPhlAn_test/TB2001B49C021_00_metagenome.txt\"\n",
    "abundances = []\n",
    "species_taxids = []\n",
    "with open(classification_tax, 'r') as f:\n",
    "    for line in f:\n",
    "        if line.startswith('#'):\n",
    "            continue\n",
    "        # if line.startswith('clade_name'):\n",
    "        #     header = line.strip().split('\\t')\n",
    "        #     continue\n",
    "        if re.search(r'\\|s__',line):\n",
    "            clade_name, lineage, abundance = line.strip().split('\\t')[:3]\n",
    "            species_name = int(lineage.strip('|').split('|')[-1])\n",
    "            species_taxids.append(species_name)\n",
    "            abundances.append(f'{float(abundance):.5f}')\n",
    "pd.Series(data=abundances,index=species_taxids).to_csv(\"/home/zhouan/Docker_dir/Classification/MetaPhlAn_test/test.tsv\", sep='\\t', header=False)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "paste <(cut -f 1 {cleaned_abundance} \\\n",
    "        | {TAXONKIT} lineage --data-dir {params.db} \\\n",
    "        | {TAXONKIT} reformat --data-dir {params.db} --format \"{{g}}\\t{{s}}\" --miss-rank-repl \"unclassified\" \\\n",
    "        | cut -f 3,4 \\\n",
    "        | awk 'BEGIN{{FS=\"\\t\"; OFS=\"\\t\"}} {{if ($1 == \"unclassified\" && $2 != \"unclassified\") {{print \"missing_genus\", $2}} else {{print}}}}' \\\n",
    "        ) \\\n",
    "        <(cut -f 2 {cleaned_abundance}) \\\n",
    "> {output.cleaned_abundance_tax};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification = \"/home/zhouan/Docker_dir/Classification/MetaPhlAn_test/TB2001B49C021_00_metagenome.txt\"\n",
    "cleaned_abundance_tax = \"/home/zhouan/Docker_dir/Classification/MetaPhlAn_test/classification_cleaned_tax.tsv\"\n",
    "TSV_metaphlan = \"/home/zhouan/Docker_dir/Classification/MetaPhlAn_test/output_metaphlan.tsv\"\n",
    "duplicate_names=  '/home/zhouan/Docker_dir/Classification/MetaPhlAn_test/corrected_duplicates_metaphlan.log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open(classification, 'r') as f:\n",
    "    for line in f:\n",
    "        if line.split('\\t')[0] == \"UNCLASSIFIED\":\n",
    "            unknown_portion = float(line.strip().split('\\t')[:3][-1])\n",
    "\n",
    "metaphlan_input = pd.read_table(cleaned_abundance_tax,\n",
    "                                names=['genus', 'species', 'abundance'])\n",
    "\n",
    "organism_count = pd.Series(metaphlan_input['abundance'].values / 100, index=metaphlan_input[\"genus\"])\n",
    "organism_count['no_hit'] = unknown_portion / 100\n",
    "\n",
    "# Sum for the same organisms. This is important with genera as there can be multiple species with the same genus.\n",
    "organism_count = organism_count.groupby(organism_count.index).sum()\n",
    "\n",
    "if any(organism_count.index.duplicated(keep=False)):\n",
    "    organism_count[organism_count.index.duplicated()].to_csv(duplicate_names, sep='\\t',index=True,header=False)\n",
    "    braken2_input=organism_count.groupby(organism_count.index,sort=False).sum()\n",
    "\n",
    "organism_count.to_csv(TSV_metaphlan, sep='\\t', index=True, header=False)\n",
    "organism_count.index.duplicated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kraken2 output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kraken2_ouput_file = \"/home/zhouan/Docker_dir/Classification/00.extract_method/Vazyme_D6300/BenchmarkingClassifiers/kraken2/test.txt\"\n",
    "import pandas as pd\n",
    "kraken2_ouput = pd.read_table(kraken2_ouput_file,\n",
    "                                header = None,\n",
    "                                usecols=[0,1,2,3]\n",
    "                                )\n",
    "## Remove unclassified maps\n",
    "kraken2_ouput_clean = kraken2_ouput[kraken2_ouput.iloc[:,0] == 'C']\n",
    "kraken2_ouput\n",
    "# kraken2_ouput_file \n",
    "\"\"\"\n",
    "C/U:classed or unclassed\n",
    "Sequence ID\n",
    "taxonomy ID\n",
    "序列长度\n",
    "序列中每个k-mer对应的LCA （空格间隔；taxid ： k-mer numbers）\n",
    "\"\"\"\n",
    "# kraken2_ouput_clean 文件有前四列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "paste <(cut -f 2,3 {input}) <(cut -f 3 {input} \\\n",
    "        | {TAXONKIT} lineage --data-dir {params.db} \\\n",
    "        | {TAXONKIT} reformat --data-dir {params.db} --format \"{{g}}\\t{{s}}\" --miss-rank-repl \"unclassified\" \\\n",
    "        | cut -f 3,4 \\\n",
    "        | awk 'BEGIN{{FS=\"\\t\"; OFS=\"\\t\"}} {{if ($1 == \"unclassified\" && $2 != \"unclassified\") {{print \"missing_genus\", $2}} else {{print}}}}' \\\n",
    "        ) > {output.TSV};"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sylph output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_tax = \"/home/zhouan/Docker_dir/Classification/00.extract_method/Vazyme_D6300/BenchmarkingClassifiers/sylph/classification_tax.tsv\"\n",
    "output_TSV = \"/home/zhouan/Docker_dir/Classification/00.extract_method/Vazyme_D6300/BenchmarkingClassifiers/sylph/classification_cleaned.tsv\"\n",
    "output_tax_TSV = \"/home/zhouan/Docker_dir/Classification/00.extract_method/Vazyme_D6300/BenchmarkingClassifiers/sylph/classification_cleaned_tax.tsv\"\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "species_names = []\n",
    "genus_names = []\n",
    "abundances = []\n",
    "relative_abundances = []\n",
    "sequence_abundances = []\n",
    "with open(classification_tax, 'r') as f:\n",
    "    for line in f:\n",
    "        if line.startswith(\"#\"):\n",
    "            continue\n",
    "        if re.search(r'\\|s__',line):\n",
    "            clade_name, relative_abundance, sequence_abundance = line.strip().split('\\t')[:3]\n",
    "            if re.search(r'\\|t__',clade_name):\n",
    "                continue\n",
    "            else:\n",
    "                species_name = clade_name.split('|')[-1].split('_')[2]\n",
    "                genus_name = clade_name.split('|')[-1].split('_')[2].split(' ')[0].split('_')[0]\n",
    "            species_names.append(species_name)\n",
    "            genus_names.append(genus_name)\n",
    "            relative_abundances.append(f'{float(relative_abundance):.5f}')\n",
    "            sequence_abundances.append(f'{float(sequence_abundance):.5f}')\n",
    "\n",
    "pd.Series(data=relative_abundances,index=species_names).to_csv(output_TSV, sep='\\t', header=False)\n",
    "cleaned_tax = list(zip(genus_names, species_names, relative_abundances))\n",
    "pd.DataFrame(cleaned_tax).to_csv(output_tax_TSV, sep='\\t', index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t', '', 'GCF', '018333375.1']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clade_name.split('|')[-1].split('_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 根据species name 找 taxid\n",
    "# cat species_names.txt | {TAXONKIT} names2taxid --data-dir {params.db} ## 输出species name\\ttaxid\n",
    "# classification_cleaned_tax.tsv # genus\\tspecies\\tabundance\n",
    "# data = list(zip(genus_names, species_names, relative_abundances))\n",
    "\n",
    "# # 将合并后的数据写入到 output_TSV 文件\n",
    "# df = pd.DataFrame(data, columns=['genus_name', 'species_name', 'relative_abundance'])\n",
    "\n",
    "# # 将 DataFrame 写入 TSV 文件，使用制表符分隔，且不包含列名（header=False）\n",
    "# df.to_csv(output_TSV, sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TB2001D03C021.fastq\n",
      "TB2001D03C021\n",
      ".gz\n",
      "TB2001D03C021.fastq_HQ.gz\n"
     ]
    }
   ],
   "source": [
    "# ROOT / 'input' / (Path(config['input']['fastq']).stem + '_HQ' + Path(config['input']['fastq']).suffix)\n",
    "fastq = \"/home/zhouan/Cyclone_data/Metagenome/meta_241108/TB2001D03C021/TB2001D03C021.fastq.gz\"\n",
    "\n",
    "from pathlib import Path\n",
    "p = Path(fastq)\n",
    "file_name = str(p.stem).split('.')[0]\n",
    "print(p.stem)   # 输出 'file'\n",
    "print(file_name) # 输出 '.fastq'\n",
    "print(p.suffix)    # 输出 'gz'\n",
    "test = p.stem + '_HQ' + p.suffix\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file.fastq\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'file.fastq.gz'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_path = \"/path/to/your/file.fastq.gz\"  # 或者 \"/path/to/your/file.fq.gz\"\n",
    "file_name = os.path.basename(file_path)  # 提取文件名\n",
    "file_base_name = os.path.splitext(file_name)[0]  # 去掉扩展名\n",
    "print(file_base_name)  # 输出 'file'\n",
    "file_name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BenchmarkingClassifiers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
